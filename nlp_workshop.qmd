---
title: "Intro to Natural Language Processing"
author: "Casey O'Hara"
format: 
  html:
    embed-resources: true
    code-folding: true
editor: visual
execute:
  warning: false
  message: false
---

# Learning Objectives

-   use `synthesisr` to read in and write out references in R
-   Work with text data in a tidy workflow, using `tidytext`
-   apply lemmatization to a corpus of documents to standardize words to their roots
-   apply a TF-IDF calculation to a corpus of documents to get a sense of relative word importance
-   apply a Latent Dirichlet Analysis to a corpus of documents to model topics within the corpus

```{r setup}
library(tidyverse)
library(tidytext)
library(synthesisr)
library(topicmodels)
library(ggwordcloud)
```

# Natural Language Processing

Words in a text carry meaning and information. Natural language processing is a field of CS focused on processing natural language data, like text or speech, to extract information, make meaning, and possibly even generate new data as a response. Early NLP was rule-based, like applying a dictionary and a grammar rulebook to parse text. Probabilistic methods using machine learning were the next big development, and that's where we'll focus. ChatGPT and other LLMs use insanely huge neural networks, a bit beyond our scope here!

For this workshop, we will use NLP to classify documents into similar categories, or "topics", based on similarities and differences among the various words found in the abstracts.

We will be considering a text as a "bag of words" model, which disregards word order and grammar. This is way easier than trying to parse complex language, but clearly misses out on a lot of information! However, for the topic modeling we're aiming for in this workshop, where we will classify documents according to their text content, it will work fine.

# Data

We will be working with a bibliography exported from Web of Science, the results of five topic-level searches on April 19, 2024, using the terms __"social ecological system*" AND [topic]__ where [topic] is one of "fisher\*", "forestry", "grazing", "marine protected area\*", or ("water resource\*" OR "irrigation").  The searches returned 1066 unique documents with abstracts that we can use for our analysis.

The `synthesisr` package is our friend here, reading and writing bibliography files in common formats - here we'll use bibtex format.

> Systematic review searches include multiple databases that export results in a variety of formats with overlap in coverage between databases. To streamline the process of importing, assembling, and deduplicating results, synthesisr recognizes bibliographic files exported from databases commonly used for systematic reviews and merges results into a standardized format.

```{r read in data}
refs_files <- list.files(here::here('data'), pattern = '.bib', full.names = TRUE)

refs_df_raw <- synthesisr::read_refs(refs_files)

refs_df <- refs_df_raw %>%
  select(type, author, title, journal, year, keywords, abstract, times_cited, src = filename) %>%
  mutate(times_cited = as.numeric(times_cited)) %>%
  filter(abstract != 'NA') %>%
  distinct(author, title, year, .keep_all = TRUE) %>%
  mutate(src = basename(src)) %>%
  mutate(doc_id = paste0('text', 1:n()))
```

# Explore the data

Let's examine word frequency in the abstracts of these documents. This is all done in more depth in the `tidytext` workshop materials, including working with pdfs and sentiment analysis. Pseudocode:

-   break the abstracts into words (tokenize)
-   eliminate common, uninformative words (stop words) and numbers (filter using regex)
-   examine results by frequency across all docs

```{r tokenize and drop stop words}
abstr_words_df <- refs_df %>%
  unnest_tokens(input = 'abstract', output = 'word') %>%
  anti_join(stop_words) %>%
  filter(!str_detect(word, '[0-9]'))
```

```{r plot frequency across all docs}
all_freq_df <- abstr_words_df %>%
  group_by(word) %>%
  summarize(n = n(), .groups = 'drop') %>%
  slice_max(n, n = 100) %>%
  mutate(angle = 45 * sample(-2:2, n(),
    replace = TRUE,
    prob = c(1, 1, 4, 1, 1)
  ))

ggplot(all_freq_df, aes(label = word, size = n)) +
  geom_text_wordcloud(aes(angle = angle)) +
  scale_size_area(max_size = 12) +
  theme_minimal()
```

Note that some terms show up multiple times with slight variation: e.g., "resources" vs. "resource", "community" vs. "communities", "fishery" vs. "fisheries". If we account for those differences actually referring to basically the same concept, how might that change the apparent importance?

# Lemmatization

A *lemma* is the root form of a word. For example, in English, the verb "to be" has many forms based on tense or person: I am, we are, she is, we were... but they have the same root. Above, "resources" and "resource" have the same root ("resource") but one is plural. If we condense words to their roots, we can get a better sense of their contextual frequency.

Unfortunately lemmatization is not easy, but there are packages and external software to do this for us. TreeTagger seems to be a big one for parsing parts of speech and lemmas for many languages, but requires installation of external software (which threw an error for me). The `spaCy` package for Python is also pretty powerful and well supported, and there's an R package to run it (requiring a Python installation etc...). Check out <https://spacy.io/> for more details.

-   Install `spaCy` (e.g., `pip install spacy`) if not already in existing install
-   download language model, (e.g., `python -m spacy download en_core_web_sm`)

For our purposes, I did this in advance in case you didn't feel like installing Python and such. Check the results - note that this lemmatization software does not simply treat the abstracts as "bags of words" but accounts for grammar/parts of speech etc. in context.

```{r}
abstr_parsed_f <- here::here('int/abstr_parsed.csv')
if(!file.exists(abstr_parsed_f)) {
  library(spacyr)
  
  # spacy_install() ### installs Python (if necessary) and everything to access the spaCy package
  
  spacy_initialize(model = 'en_core_web_sm') 
  
  system.time(abstr_parsed_df <- spacy_parse(refs_df$abstract))

  write_csv(abstr_parsed_df, abstr_parsed_f)
}

abstr_parsed_df <- read_csv(abstr_parsed_f)
```

## check results post-lemmatization

We could at this point redo the word frequency stuff above, including word cloud to see how lemmatization affects the prevalence certain terms. Here we can drop the stop words and punctuation (which is retained by the parser) and a few other things to clean up.

```{r plot wordcloud after lemmatization}
lemma_freq_df <- abstr_parsed_df %>%
  anti_join(stop_words, by = c('lemma' = 'word')) %>%
  filter(pos != 'PUNCT') %>%
  filter(!str_detect(lemma, '[0-9\\%]')) %>%
  filter(nchar(lemma) > 2) %>%
  group_by(lemma) %>%
  summarize(n = n(), .groups = 'drop') %>%
  slice_max(n, n = 100) %>%
  mutate(angle = 45 * sample(-2:2, n(),
    replace = TRUE,
    prob = c(1, 1, 4, 1, 1)
  ))

ggplot(lemma_freq_df, aes(label = lemma, size = n)) +
  geom_text_wordcloud(aes(angle = angle)) +
  scale_size_area(max_size = 12) +
  theme_minimal()
```

# Term-frequency X inverse doc frequency

A common metric to assess the importance and uniqueness of a word to a document in a body of documents is the product of *term frequency* and *inverse document frequency*, or *TF-IDF*. This can help us find key differences and similarities between documents, to classify them

-   Term frequency is relative frequency of term $t$ in document $d$. This is an indication of how important the term is to that specific document - this will range from 0 (term doesn't show up at all) to 1 (every word is that term). $$\text{tf}(t, d) = \frac{f_{t,d}}{\sum_{t'\in d}f_{t',d}}$$
-   Inverse document frequency is how common or rare the term $t$ is across all documents, by counting how many documents it shows up in, $n_t$, across all documents $N$. Rare words, those that show up in only a few documents, are helpful for identifying similar documents, while common words that show up in almost everything don't help differentiate among documents. Words that show up in literally everything will have a zero! So, a rare word gets a higher score here.\
    $$\text{idf}(t, D) = \log\frac{N}{n_t} = -\log\frac{n_t}{N}$$

The product of these helps identify words that are important *within* an abstract, but uncommon *across* all abstracts.

Let's apply this to our data!

```{r}
abstr_tf <- abstr_parsed_df %>%
  group_by(doc_id) %>%
  mutate(word_ct = n()) %>%
  group_by(doc_id, lemma) %>%
  summarize(word_ct = first(word_ct),
            term_ct = n(),
            tf = term_ct / word_ct,
            .groups = 'drop')

abstr_idf <- abstr_parsed_df %>%
  mutate(N = n_distinct(doc_id)) %>%
  group_by(lemma) %>%
  summarize(n_t = n_distinct(doc_id),
            N = first(N),
            idf = log(N / n_t))

abstr_tfidf <- abstr_tf %>% 
  left_join(abstr_idf, by = 'lemma') %>%
  mutate(tfidf = tf * idf)
```

Note that certain words float to the top for certain documents, giving a sense of what might differentiate that document from all the others in the corpus of text. While there is some cleaning up to do, note how the product eliminates or nearly eliminates stop words for us! Though of course, we can do that manually (as well as the punctuation and numbers).

# Latent Dirichlet Allocation

LDA is a handy method for classifying documents without any additional info other than the texts themselves. In other words, it is *unsupervised* machine learning. It can cluster together documents according to similarities in term frequency between them, and differences in term frequency compared to documents in other clusters.

One way to think of it: each document is a probabilistic mixture of *one or more* topics. For our documents based on social-ecological systems, some topics might be fisheries, forestry, water systems, or comanagement. For each of these topics, certain words will be more or less strongly associated with it - for these topics, perhaps "water" is strongly associated with fisheries and water systems, but weakly associated with forestry. We don't know the topics in advance (thus "latent") but we can cluster according to word associations among the documents, and then look at those word associations to tease out what the topics might be.

## Get data into proper format

First, let's start with our dataframe of lemmatized abstracts, but clean it up a bit since more words (including uninformative stop words) makes it take a lot longer.  Then we will cast this to a "Document Term Matrix" that summarizes how many times each word shows up in each document (similar, but not the same, as TF-IDF) - each row a document, each column a word, and each cell the number of times that word shows up in that document.  

```{r}
abstr_clean_df <- abstr_parsed_df %>%
  select(doc_id, lemma) %>%
  mutate(lemma = tolower(lemma)) %>%
  mutate(lemma = str_remove_all(lemma, '[^a-z]')) %>%
  anti_join(stop_words, by = c('lemma' = 'word')) %>%
  filter(nchar(lemma) > 2) %>%
  group_by(doc_id, lemma) %>%
  summarize(n = n(), .groups = 'drop')

abstr_dtm <- cast_dtm(data = abstr_clean_df, 
                      term = lemma, 
                      document = doc_id,
                      value = n)
```

## How many topics?

Next we'll want to feed that document-term matrix to the LDA function to identify topic clusters.  But first, we need to know how many clusters we would like it to find!  The `ldatuning` package runs four different metrics to identify a good number of topics.  This takes a while (though less than manually trying each combination of k) so I've run it ahead of time and saved the results.

```{r}
tuning_f <- here::here('int/topic_tuning.csv')
if(!file.exists(tuning_f)) {
  ### remotes::install_github("nikita-moor/ldatuning")
  library(ldatuning)
  
  k_vec <- c(2:10, seq(12, 20, 2), seq(25, 40, 5))
  
  optim_topics <- FindTopicsNumber(
    abstr_dtm, 
    topics = k_vec, 
    metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
    control = list(seed = 12345),
    mc.cores = 4,
    verbose = TRUE
  )
  write_csv(optim_topics, tuning_f)
}

optim_topics <- read_csv(tuning_f)
ldatuning::FindTopicsNumber_plot(optim_topics)
```

Arun and Griffiths indices always seem to recommend MORE topics based on strict minimization/maximization, though the marginal improvement tapers off after the first five or ten.  The Deveaud index looks like it recommends ten topics, which seems better for a coarse understanding.  The CaoJuan index drops sharply to four, then less steeply to 10.  Based on these, let's choose 10 topics.



## Running LDA on the full subset

Now we can run our LDA and get some results!  This might take a little while...  Annoyingly, even though we set a random number seed, this process results in a different clustering each time!

```{r}
set.seed(42)
abstr_lda <- topicmodels::LDA(abstr_dtm, k = 10)
```

With this, we can now look at which topics were identified for which articles (the probability weighting for each topic within each article), and also the terms most important to defining those topics (the probability weighting for each term within each topic)...

```{r}
### Extract topics for each doc by probability
abstr_topics <- posterior(abstr_lda)$topic

abstr_topics <- posterior(abstr_lda)$topic %>%
  as.data.frame() %>%
  setNames(paste0('topic', names(.))) %>%
  mutate(doc_id = rownames(.))

write_csv(abstr_topics, here::here('output/topic_by_doc.csv'))

### Extract terms for each topic by probability
abstr_terms  <- posterior(abstr_lda)$term

abstr_terms  <- posterior(abstr_lda)$term %>%
  as.data.frame() %>%
  mutate(topic = paste('topic', rownames(.))) %>%
  pivot_longer(cols = -topic, names_to = 'term', values_to = 'prob') %>%
  filter(prob > 0.0001)

write_csv(abstr_terms, here::here('output/term_by_topic.csv'))
```

# Summarize topic modeling!

As a final step, I took the top 20 terms from each topic, and asked ChatGPT to suggest a brief topic name for each.

```{r}
top20 <- abstr_terms %>%
  group_by(topic) %>%
  slice_max(order_by = prob, n = 20)
write_csv(top20, here::here('output/term_by_topic_top20.csv'))
```

But, note again that LDA is not deterministic, so even with a random generator seed, the results are different every time.  For a test run (see the output folder), based on the top 20 terms for each topic and their associated probabilities, ChatGPT suggested:

* Topic 1: Water Resource Governance
* Topic 2: Dynamic Water System Management
* Topic 3: Urban Ecological Resilience
* Topic 4: Climate Change Impact and Adaptation
* Topic 5: Community Resilience in Land Management
* Topic 6: Marine Ecosystem and Fishery Management
* Topic 7: Social-Ecological Frameworks for Research
* Topic 8: Stakeholder-Centric Conservation Planning
* Topic 9: Ecosystem Services in Landscape Conservation
* Topic 10: Sustainable Water Management Modeling

Now let's join the topics and the titles to see whether these make sense!  While some documents might touch on multiple topics, for simplicity here let's just assign the most likely topic to each document.

```{r}
topic_names <- c(topic1 = 'Water Resource Governance',
                 topic2 = 'Dynamic Water System Management',
                 topic3 = 'Urban Ecological Resilience',
                 topic4 = 'Climate Change Impact and Adaptation',
                 topic5 = 'Community Resilience in Land Management',
                 topic6 = 'Marine Ecosystem and Fishery Management',
                 topic7 = 'Social-Ecological Frameworks for Research',
                 topic8 = 'Stakeholder-Centric Conservation Planning',
                 topic9 = 'Ecosystem Services in Landscape Conservation',
                 topic10 = 'Sustainable Water Management Modeling')

abstr_topics_testrun <- read_csv(here::here('output/topic_by_doc_testrun.csv'))
doc_topics <- abstr_topics_testrun %>%
  pivot_longer(-doc_id, names_to = 'topic', values_to = 'prob') %>%
  group_by(doc_id) %>%
  slice_max(order_by = prob, n = 1) %>%
  mutate(topic_desc = topic_names[topic])

topics_titles <- refs_df %>%
  filter(!is.na(abstract)) %>%
  select(doc_id, title, src) %>%
  left_join(doc_topics, by = 'doc_id') %>%
  select(-doc_id) %>%
  mutate(prob = round(prob, 3))

ggplot(topics_titles, aes(y = topic_desc, fill = src)) +
  geom_bar() +
  theme_minimal() +
  theme(axis.title.y = element_blank())

DT::datatable(topics_titles)
```

